{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import private\n",
    "\n",
    "# User Variables\n",
    "ETHERSCAN_API_KEY = private.etherscan_api\n",
    "CONTRACT_ADDRESS = \"0x827922686190790b37229fd06084350E74485b72\"\n",
    "ETHERSCAN_V2_API = 'https://api.etherscan.io/v2/api'\n",
    "FILTER_ADDRESSES = [private.wal_lp.lower(), private.sickle_lp.lower()]\n",
    "METHOD_ID_MAP = {\n",
    "    \"0x2812d614\": \"Compound\",\n",
    "    \"0xf5304377\": \"Deposit\",\n",
    "    \"0x5ec5999e\": \"Harvest\",\n",
    "    \"0xe6fb317f\": \"RebalanceFor\",\n",
    "    \"0xe5bacdd0\": \"Increase\",\n",
    "    \"0x1c396db6\": \"Decrease\",\n",
    "    \"0x28734381\": \"Exit\",\n",
    "    \"0x659b91b1\": \"Rebalance\",\n",
    "    \"0x22451262\": \"Move\"\n",
    "}\n",
    "TOPIC_ID = \"0xbf9d03ac543e8f596c6f4af5ab5e75f366a57d2d6c28d2ff9c024bd3f88e8771\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch NFT transfers\n",
    "def get_transactions(start_block, api_key, address, action='tokentx'):\n",
    "    # Just pull for given token IDs (based on Contract Address)\n",
    "        params = {\n",
    "            'chainid': '8453',  # base mainnet = 8453\n",
    "            'module': 'account',\n",
    "            'action': action,\n",
    "            # 'contractaddress': contract_address,\n",
    "            'address': address,\n",
    "            'page': '1',\n",
    "            'offset': '10000',\n",
    "            'startblock': start_block,\n",
    "            'endblock': 99999999,\n",
    "            'sort': 'asc',\n",
    "            'apikey': api_key\n",
    "        }\n",
    "\n",
    "        response = requests.get(f'{ETHERSCAN_V2_API}', params=params)\n",
    "        data = response.json()\n",
    "        if data[\"status\"] != \"1\":\n",
    "            print(\"Error fetching data:\", data[\"message\"], data['result'])\n",
    "            return []\n",
    "        \n",
    "        return data[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "nft_transactions = get_transactions(25707200, private.etherscan_api, private.sickle_lp, 'tokennfttx')\n",
    "token_transactions = get_transactions(25707200, private.etherscan_api, private.sickle_lp, 'tokentx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch transaction details and extract method ID\n",
    "def fetch_transaction_details(tx_hash, api_key):\n",
    "    url = f\"{ETHERSCAN_V2_API}?chainid=8453&module=proxy&action=eth_getTransactionByHash&txhash={tx_hash}&apikey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    if \"result\" in data and \"input\" in data[\"result\"]:\n",
    "        method_id = data[\"result\"][\"input\"][:10]  # First 10 chars are the method ID\n",
    "        token_id_pos = 64 * 3 + 10\n",
    "        token_id = int(data[\"result\"][\"input\"][token_id_pos:token_id_pos + 64], 16)\n",
    "        x = METHOD_ID_MAP.get(method_id.lower(), \"Unknown\")\n",
    "        print(f\"{x} occured at tx {tx_hash[:6]}...{tx_hash[-4:]}\")\n",
    "        return x, token_id\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = fetch_transaction_details(\"0x8c3e050eb6dd997be5556c4524e2e7646248b66bde45607bb017860da6ebe81c\", private.etherscan_api)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_token_transfers(transfers, nft_df):\n",
    "    df = pd.DataFrame(transfers)\n",
    "    df = df[[\"blockNumber\", \"timeStamp\", \"hash\", \"from\", \"to\", \"value\", \"tokenSymbol\", \"tokenDecimal\"]]\n",
    "    df[\"timeStamp\"] = pd.to_datetime(pd.to_numeric(df[\"timeStamp\"], errors='coerce'), unit='s')\n",
    "    df[\"value\"] = df[\"value\"].astype(float) / (10 ** df[\"tokenDecimal\"].astype(int))  # Convert to standard token value\n",
    "    \n",
    "    # Filter only transactions where both 'from' and 'to' are in FILTER_ADDRESSES\n",
    "    df = df[df[\"from\"].isin(FILTER_ADDRESSES) & df[\"to\"].isin(FILTER_ADDRESSES)]\n",
    "    \n",
    "    # Link token transfers to NFT burn and mint transactions\n",
    "    df = df.merge(nft_df[['hash', 'seriesID', 'eventType', 'tokenID']], on='hash', how='left')\n",
    "\n",
    "    # Remove duplicate token transfers within the same transaction hash\n",
    "    df = df.drop_duplicates(subset=[\"hash\", \"from\", \"to\", \"value\", \"tokenSymbol\"])\n",
    "\n",
    "    # Assign method ID event type for missing values\n",
    "    # Get a list of hashes that don't have an event type yet\n",
    "    tx_hashes = df[pd.isna(df[\"eventType\"])][\"hash\"].unique()\n",
    "    # Fetch the event type for each hash\n",
    "    event_types = {}\n",
    "    token_ids = {}\n",
    "    for hash in tx_hashes:\n",
    "        event_types[hash], token_ids[hash] = fetch_transaction_details(hash, ETHERSCAN_API_KEY)\n",
    "        \n",
    "    # Apply the event type to the data frame by matching the hash value\n",
    "    df[\"eventType\"] = df.apply(lambda row: event_types.get(row[\"hash\"], row[\"eventType\"]), axis=1)\n",
    "    df[\"tokenID\"] = df.apply(lambda row: token_ids.get(row[\"hash\"], row[\"tokenID\"]), axis=1)\n",
    "    df[\"tokenID\"] = df[\"tokenID\"].astype(int)\n",
    "\n",
    "    # Fill missing seriesID using tokenID mapping\n",
    "    token_to_series = df.dropna(subset=[\"seriesID\"]).set_index(\"tokenID\")[\"seriesID\"].to_dict()\n",
    "    df[\"seriesID\"] = df[\"seriesID\"].fillna(df[\"tokenID\"].map(token_to_series))\n",
    "    \n",
    "    df.sort_values(by=[\"seriesID\", \"timeStamp\"], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process transactions and track series\n",
    "def process_transactions(transactions, token_transfers):\n",
    "    df = pd.DataFrame(transactions)\n",
    "    df = df[[\"blockNumber\", \"timeStamp\", \"hash\", \"from\", \"to\", \"tokenID\"]]\n",
    "    df[\"timeStamp\"] = pd.to_datetime(pd.to_numeric(df[\"timeStamp\"], errors='coerce'), unit='s')\n",
    "    \n",
    "    # Identify burns (to burn address) and mints (from address is zero address)\n",
    "    burn_address = \"0x0000000000000000000000000000000000000000\"\n",
    "    df = df[df[\"from\"].eq(burn_address) | df[\"to\"].eq(burn_address)]  # only care about mint and burn, not transfers to the Aerodrome farms\n",
    "    df[\"eventType\"] = df[\"from\"].apply(lambda x: \"Mint\" if x == burn_address else \"Burn\")\n",
    "    \n",
    "    # Identify NFT series\n",
    "    series_map = {}\n",
    "    token_df = pd.DataFrame(token_transfers)\n",
    "    for _, row in token_df.iterrows():\n",
    "        if row[\"from\"].lower() == private.wal_lp.lower():\n",
    "            tx_hash = row[\"hash\"]\n",
    "            # Extract rows from token_df with the same hash value\n",
    "            subset = token_df[token_df[\"hash\"] == tx_hash]\n",
    "            tokens = subset[\"tokenSymbol\"].tolist()\n",
    "            # Create string out of the first two items in tokens list\n",
    "            seriesID = '/'.join(map(str, tokens[:2])) + '_' + tx_hash[-4:]\n",
    "            matching_nft_mints = df[(df[\"hash\"] == tx_hash) & (df[\"eventType\"] == \"Mint\")]\n",
    "            for _, mint in matching_nft_mints.iterrows():\n",
    "                series_map[mint[\"tokenID\"]] = seriesID\n",
    "    \n",
    "    # Assign series ID to subsequent burns and mints\n",
    "    df[\"seriesID\"] = df[\"tokenID\"].map(series_map)\n",
    "    for _, row in df.iterrows():\n",
    "        if row[\"eventType\"] == \"Burn\" and row[\"tokenID\"] in series_map:\n",
    "            tx_hash = row[\"hash\"]\n",
    "            new_mint = df[(df[\"hash\"] == tx_hash) & (df[\"eventType\"] == \"Mint\")]\n",
    "            for _, mint in new_mint.iterrows():\n",
    "                series_map[mint[\"tokenID\"]] = series_map[row[\"tokenID\"]]\n",
    "\n",
    "    # Get unique list of all the transaction hash values\n",
    "    tx_hashes = df[\"hash\"].unique()\n",
    "    # Fetch the event type for each hash\n",
    "    event_types = {}\n",
    "    for hash in tx_hashes:\n",
    "        event_types[hash], id = fetch_transaction_details(hash, ETHERSCAN_API_KEY)\n",
    "    # Apply the event type to the data frame by matching the hash value\n",
    "    df[\"eventType\"] = df[\"hash\"].map(event_types)\n",
    "\n",
    "    df[\"seriesID\"] = df[\"tokenID\"].map(series_map)\n",
    "    df.sort_values(by=[\"seriesID\", \"timeStamp\"], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nft = process_transactions(nft_transactions, token_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harvest occured at tx 0x64c7...bd0e\n",
      "Harvest occured at tx 0x3083...4dc9\n",
      "Harvest occured at tx 0x49da...dcec\n",
      "Harvest occured at tx 0xb69d...6d35\n",
      "Compound occured at tx 0x8584...2d99\n",
      "Harvest occured at tx 0xf7b3...5322\n",
      "Harvest occured at tx 0xbe32...1dc1\n",
      "Harvest occured at tx 0xcc40...363b\n",
      "Harvest occured at tx 0xf05a...f773\n",
      "Increase occured at tx 0x8c3e...e81c\n",
      "Harvest occured at tx 0xd324...1fac\n"
     ]
    }
   ],
   "source": [
    "tokens = process_token_transfers(token_transactions, nft)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
